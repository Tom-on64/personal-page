# The making of Neon #2
---

In the [first part](/logs/log.html?log=neon1) of this series i explained how a compiler works (in general) and now, we're going to write a simple one. I'll be making it in TypeScript, but you can follow in your language of choice.

Our goal for the first few parts of this series will be to compile the most simple program:
```Neon?
return 0;
```
All this program does is exits with the exit code of 0.
Later i will add a main function, but for now it will be in the top-scope.

## The Lexer
The Lexer is the first part of a compiler. It's job is to take the input code and convert it into tokens.

## What is a token?
In Neon, a token is an object that has a type and an optional value.

Here's the token interface definition:
\`\`\`typescript
interface IToken {
    type: TokenType;
    value?: string | number;
};
\`\`\`

You may have noticed the 'TokenType'. That is an enum of all the valid token types.
For now it can look like this:
\`\`\`typescript
enum TokenType {
    IDENTIFIER: "ident", 
    RETURN = "stmtReturn", 
    INT_LIT = "LInt", 
    EOL = "EOL", // End of line (semicolon)
    EOF = "EOF", // End of file
};
\`\`\`

## Lexical Analisis
Now what does that mean? Well while it may sound complex, it's just the process of converting some input text into tokens. We've already defined how tokens will look so now let's write our lexer.

I'm going to make a class for the Lexer and it will look like this:
\`\`\`Typescript
class Lexer {
    private index = 0;
    private input = [''];

    public tokenize(inputCode: string): IToken[] {
        // Split our input into individual characters
        this.input = inputCode.split('');

        const tokens: IToken[] = [];

        // Loop through our input
        while (this.index < this.input.length) {
            // TODO: Actually tokenize the code
        }

        return tokens;
    }
}
\`\`\`

So that is our basic Lexer class, but we still need to tokenize the input.
We'll do that, but first we need to define some helper functions. We're going to have three:
- current()
- consume()
- peek()

Here's how they'll be implemented:
\`\`\`Typescript
class Lexer {
    // TODO: Add error detection, when the index is larger than our input arrays lenght
    private current(): string {
        return this.input[this.index]; // Return the current character
    }

    private consume(amount = 1): string {
        this.index += amount;
        return this.input[this.index - amount]; // Return the consumed character
    }

    private peek(amount = 1): string {
        return this.input[this.index + amount]; // Return the peeked character 
    }

    // ...Our other code
}
\`\`\`

Okay, now we can come back to the while loop in our tokenize method:
\`\`\`Typescript
class Lexer {
    // ...Other code

    public tokenize(inputCode: string): IToken[] {
        // Split our input into individual characters
        this.input = inputCode.split('');

        const tokens: IToken[] = [];

        // Loop through our input
        while (this.index < this.input.length) {
            if (this.current().match(/\s/)) this.consume(); // Ignore whitespace
            else if (this.current().match(/[A-Za-z_]/)) { // An Identifier (Starts with a letter or '_'. Cannot start with a number)
                let identifier = this.consume(); // Declare our token
                while (this.current().match(/[A-Za-z0-9_]/)) identifier += this.consume(); // Loop until the token ends
                const token: IToken = { type: TokenType.IDENTIFIER, value: identifier };

                // Check if the token is a keyword
                if (identifier === "return") token.type = TokenType.RETURN;
                tokens.push(token)
            } else if (this.current().match(/[0-9]/)) { // An integer
                let numString = this.consume();
                while (this.current().match(/[0-9]/)) numString += this.consume();
                tokens.push({ type: TokenType.INT_LIT, value: parseInt(numString) }); // Add the integer
            } else if (this.current() === ";") { // Get a semicolon
                tokens.push({ type: TokenType.EOL });
                this.consume();
            } else console.error(`Unexpected character '${this.consume()}'.`); // Print an error
        }

        tokens.push({ type: TokenType.EOF }); // Add an end of file token

        return tokens;
    }
}
\`\`\`

Okay, that should be it!

So let's recap, we defined out token and token types, then we made out Lexer class, that has a tokenize() method which returns an array of tokens, then we made some helper methods in the Lexer and lastly, we wrote the code which loops through the input code, and converts it into tokens.

Now if you were to input this code:
```Neon
return 0;
```

You should get these tokens:
```json
[
    { "type": "return" }, 
    { "type": "LInt", "value": 0 }, 
    { "type": "EOL" }, 
    { "type": "EOF" }
]
```

## That's it!
This is going to be it for this tutorial? (i'm not sure yet).
If you want to, you can see Neon [here](https://github.com/Tom-on64/neon).

So bye!
